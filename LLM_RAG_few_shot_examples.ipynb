{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n",
    "# Use Watsonx to respond to natural language questions using RAG approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Please note that for the watsonx challenge, please consider running these notebooks locally on your laptop/desktop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This notebook contains the steps and code to demonstrate support of Retrieval Augumented Generation in watsonx.ai. It introduces commands for data retrieval, knowledge base building & querying, and model testing.\n",
    "\n",
    "This notebook uses Python 3.10.\n",
    "\n",
    "#### Objective\n",
    "\n",
    "Use LLM model, Langchain and Milvus to create a Retrieval Augmented Generation (RAG) system. This will allow us to ask questions about our documents (that were not included in the training data), without fine-tunning the Large Language Model (LLM). When using RAG, if you are given a question, you first do a retrieval step to fetch any relevant documents from a special database, a vector database where these documents were indexed.\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is a versatile pattern that can unlock a number of use cases requiring factual recall of information, such as querying a knowledge base in natural language.\n",
    "\n",
    "##### Definitions:\n",
    "\n",
    "* LLM - Large Language Model\n",
    "* Langchain - a framework designed to simplify the creation of applications using LLMs\n",
    "* Vector database - a database that organizes data through high-dimmensional vectors\n",
    "* Milvus - vector database\n",
    "* RAG - Retrieval Augmented Generation (see below more details about RAGs)\n",
    "\n",
    "#### What is a Retrieval Augmented Generation (RAG) system?\n",
    "Large Language Models (LLMs) has proven their ability to understand context and provide accurate answers to various NLP tasks, including summarization, Q&A, when prompted. While being able to provide very good answers to questions about information that they were trained with, they tend to hallucinate when the topic is about information that they do \"not know\", i.e. was not included in their training data. Retrieval Augmented Generation combines external resources with LLMs. The main two components of a RAG are therefore a retriever and a generator.\n",
    "\n",
    "The retriever part can be described as a system that is able to encode our data so that can be easily retrieved the relevant parts of it upon queriying it. The encoding is done using text embeddings, i.e. a model trained to create a vector representation of the information. The best option for implementing a retriever is a vector database. As vector database, there are multiple options, both open source or commercial products. Few examples are ChromaDB, Mevius, FAISS, Pinecone, Weaviate. Our option in this Notebook will be a local instance of ChromaDB (persistent).\n",
    "\n",
    "For the generator part, the obvious option is a LLM. In this Notebook we will use a quantized LLaMA v2 model, from the Kaggle Models collection.\n",
    "\n",
    "The orchestration of the retriever and generator will be done using Langchain. A specialized function from Langchain allows us to create the receiver-generator in one line of code.\n",
    "\n",
    "In its simplest form, RAG requires 3 steps:\n",
    "\n",
    "- Index knowledge base passages (once)\n",
    "- Retrieve relevant passage(s) from knowledge base (for every user query)\n",
    "- Generate a response by feeding retrieved passage into a large language model (for every user query)\n",
    "\n",
    "\n",
    "#### Data:\n",
    "\n",
    "##### Knowledge base:\n",
    "690,000 words worth of cleaned text from Wikipedia.\n",
    "\n",
    "##### Train and Test datasets: \n",
    "There are three question files, one for each year of students: S08, S09, and S10.\n",
    "\n",
    "The \"questionanswerpairs.txt\" files contain both the questions and answers. The columns in this file are as follows:\n",
    "\n",
    "- ArticleTitle is the name of the Wikipedia article from which questions and answers initially came.\n",
    "- Question is the question.\n",
    "- Answer is the answer.\n",
    "- DifficultyFromQuestioner is the prescribed difficulty rating for the question as given to the question-writer.\n",
    "- DifficultyFromAnswerer is a difficulty rating assigned by the individual who evaluated and answered the question, which may differ from the difficulty in field 4.\n",
    "- ArticleFile is the name of the file with the relevant article.\n",
    "\n",
    "Questions that were judged to be poor were discarded from this data set.\n",
    "\n",
    "There are frequently multiple lines with the same question, which appear if those questions were answered by multiple individuals. https://www.kaggle.com/rtatman/questionanswer-dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"setup\"></a>\n",
    "##  Set up the environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Install and import dependecies\n",
    "\n",
    "**Note:** For Windows environments, please remove `| tail -n 1` commands in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install sentence_transformers | tail -n 1\n",
    "#!pip install pandas | tail -n 1\n",
    "#!pip install rouge_score | tail -n 1\n",
    "#!pip install nltk | tail -n 1\n",
    "#!pip install \"ibm-watson-machine-learning>=1.0.312\" | tail -n 1\n",
    "#!pip install PyPDF2 | tail -n 1\n",
    "#!pip install langchain | tail -n 1\n",
    "#!pip install --upgrade pip | tail -n 1\n",
    "#!pip install ibm-generative-ai | tail -n 1\n",
    "#!pip install ipywidgets | tail -n 1\n",
    "#!pip install ipywidgets widgetsnbextension pandas-profiling | tail -n 1\n",
    "#!jupyter nbextension enable --py widgetsnbextension\n",
    "#!python3 -m pip install tensorflow-macos | tail -n 1\n",
    "#!python -m pip install tensorflow-metal | tail -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install milvus with [client] extras by pip\n",
    "#!python3 -m pip install \"milvus[client]\" | tail -n 1\n",
    "#!pip install -U --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Please restart the notebook kernel to pick up proper version of packages installed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(0)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from milvus import default_server\n",
    "from pymilvus import (\n",
    "    Collection,\n",
    "    CollectionSchema,\n",
    "    DataType,\n",
    "    FieldSchema,\n",
    "    connections,\n",
    "    utility,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Milvus\n",
    "\n",
    "from genai import Model\n",
    "from genai.model import Credentials\n",
    "from genai.schemas import GenerateParams\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Watsonx API connection\n",
    "This cell defines the credentials required to work with watsonx API for Foundation\n",
    "Model inferencing.\n",
    "\n",
    "**Action:** Provide the IBM Cloud user API key. For details, see\n",
    "[documentation](https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "BAM_API_URL = os.getenv(\"BAM_API_URL\")\n",
    "BAM_API_KEY = os.getenv(\"BAM_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "credentials = Credentials(api_key=BAM_API_KEY, api_endpoint=BAM_API_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Defining the project id\n",
    "The API requires project id that provides the context for the call. We will obtain the id from the project in which this notebook runs. Otherwise, please provide the project id.\n",
    "\n",
    "**Hint**: You can find the `project_id` as follows. Open the prompt lab in watsonx.ai. At the very top of the UI, there will be `Projects / <project name> /`. Click on the `<project name>` link. Then get the `project_id` from Project's Manage tab (Project -> Manage -> General -> Details).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    project_id = os.getenv('PROJECT_ID')\n",
    "except KeyError:\n",
    "    pproject_id = input(\"Please enter your project_id (hit enter): \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"data\"></a>\n",
    "## Train and Test data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load train and test datasets. At first, training dataset (`train_data`) should be used to work with the models to prepare and tune prompt. Then, test dataset (`test_data`) should be used to calculate the metrics score for selected model, defined prompts and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "(1715, 6)\n",
      "---------------\n",
      "(825, 6)\n",
      "---------------\n",
      "(1458, 6)\n"
     ]
    }
   ],
   "source": [
    "df_08 = pd.read_table('/Users/rrfsantos/Documents/GitHub/portifolio/S08_question_answer_pairs.txt')\n",
    "df_09 = pd.read_table('/Users/rrfsantos/Documents/GitHub/portifolio/S09_question_answer_pairs.txt')\n",
    "df_10 = pd.read_table('/Users/rrfsantos/Documents/GitHub/portifolio/S10_question_answer_pairs.txt', \n",
    "                      encoding = 'windows-1252')  #engine = 'python', error_bad_lines = False)\n",
    "\n",
    "print('-' * 15, df_08.shape, sep='\\n')\n",
    "print('-' * 15, df_09.shape, sep='\\n')\n",
    "print('-' * 15, df_10.shape, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "(3420, 3)\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([df_08, df_09, df_10], ignore_index=True)\n",
    "df.drop(['DifficultyFromQuestioner', 'DifficultyFromAnswerer', 'ArticleTitle'], axis = 1, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print('-' * 15, df.shape, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Question        0\n",
      "Answer          0\n",
      "ArticleFile     0\n",
      "answer_clean    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Limpar coluna \"Answer\"\n",
    "def strip_last_punctuation(s):\n",
    "  if s and s[-1] in string.punctuation:\n",
    "    return s[:-1].strip()\n",
    "  else:\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "df['answer_clean'] = df['Answer'].str.lower().map(strip_last_punctuation)\n",
    "\n",
    "# Remove os dados faltantes da base de treino\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print('-' * 15, df.isna().sum(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "(2783, 3)\n",
      "---------------\n",
      "Question        0\n",
      "ArticleFile     0\n",
      "answer_clean    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df.drop(['Answer'], axis = 1, inplace=True)\n",
    "df.drop_duplicates(subset=['answer_clean', 'Question'], keep='last', inplace = True)\n",
    "print('-' * 15, df.shape, sep='\\n')\n",
    "print('-' * 15, df.isna().sum(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Build up knowledge base\n",
    "\n",
    "The current state-of-the-art in RAG is to create dense vector representations of the knowledge base in order to calculate the semantic similarity to a given user query.\n",
    "\n",
    "We can generate dense vector representations using embedding models. In this notebook, we use [SentenceTransformers](https://www.google.com/search?client=safari&rls=en&q=sentencetransformers&ie=UTF-8&oe=UTF-8) [all-distilroberta-v1](https://huggingface.co/sentence-transformers/all-distilroberta-v1) to embed both the knowledge base passages and user queries.\n",
    "\n",
    "A vector database is optimized for dense vector indexing and retrieval. This notebook uses [Milvus](https://milvus.io), a user-friendly open-source vector database, licensed under Apache 2.0, which offers good speed and performance with all-MiniLM-L6-v2 embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The size of each passage is limited by the embedding model's context window (which is 512 tokens for `all-distilroberta-v1`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load knowledge base documents\n",
    "\n",
    "##### Cleaned text from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate a dataframe with information about the docs available \n",
    "\n",
    "files = glob.glob('/Users/rrfsantos/Documents/GitHub/portifolio/context/*.clean')\n",
    "\n",
    "list_text = []\n",
    "list_file = []\n",
    "\n",
    "for file in files:\n",
    "  with open(file, 'r', encoding = 'utf-8', errors='ignore') as f:\n",
    "    text = f.read()\n",
    "  list_text.append(re.sub(r'\\n+', ' ', text).strip())\n",
    "  list_file.append((os.path.basename(file)).split('.')[0])\n",
    "df = pd.DataFrame({'title': list_file,'text': list_text})\n",
    "\n",
    "missing_cols, missing_rows = ((df.isnull().sum(x) | df.eq(' ').sum(x))\n",
    "        .loc[lambda x: x.gt(0)].index\n",
    "        for x in (0, 1))\n",
    "\n",
    "df.loc[missing_rows, missing_cols]\n",
    "df['id'] = df.reset_index().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create an embedding function\n",
    "\n",
    "Note that you can feed a custom embedding function to be used by Milvus. The performance of Milvus may differ depending on the embedding model used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a Sentence Transformer: https://www.sbert.net/docs/pretrained_models.html\n",
    "embedding_model = SentenceTransformer(\"all-distilroberta-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Milvus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the Milvus embedded server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server is already running\n"
     ]
    }
   ],
   "source": [
    "if default_server.running != True:\n",
    "    default_server.start()\n",
    "    print(\"Server should have now started\")\n",
    "else:\n",
    "    default_server.stop()\n",
    "    default_server.cleanup()\n",
    "    default_server.start()\n",
    "    print(\"Server is already running\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish a connection with the embedded server and print its version information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2.3.3-lite\n"
     ]
    }
   ],
   "source": [
    "connections.connect(host=\"localhost\", port=default_server.listen_port)\n",
    "print(utility.get_server_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = \"wikipedia_collection\"\n",
    "INDEX_NAME = \"wikipedia_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection has been deleted\n"
     ]
    }
   ],
   "source": [
    "# Run if you want to drop your old data\n",
    "try:\n",
    "    utility.drop_collection(COLLECTION_NAME)\n",
    "    print(\"Collection has been deleted\")\n",
    "except:  # noqa: E722\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = FieldSchema(\n",
    "    name=\"id\",\n",
    "    dtype=DataType.INT64,\n",
    "    is_primary=True,\n",
    "    auto_id=True,\n",
    ")\n",
    "\n",
    "text = FieldSchema(\n",
    "    name=\"text\",\n",
    "    dtype=DataType.VARCHAR,\n",
    "    max_length=6000,\n",
    ")\n",
    "\n",
    "text_vector = FieldSchema(name=\"text_vector\", dtype=DataType.FLOAT_VECTOR, dim=768)\n",
    "\n",
    "qid = FieldSchema(name=\"qid\", dtype=DataType.INT64)\n",
    "\n",
    "title = FieldSchema(\n",
    "    name=\"title\",\n",
    "    dtype=DataType.VARCHAR,\n",
    "    max_length=5120,\n",
    ")\n",
    "\n",
    "schema = CollectionSchema(\n",
    "    fields=[id, text, text_vector, qid, title],\n",
    "    description=\"SIEM vector store\",\n",
    "    enable_dynamic_field=True,\n",
    ")\n",
    "\n",
    "collection = Collection(\n",
    "    name=COLLECTION_NAME, schema=schema, using=\"default\", shards_num=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed and index documents with Milvus\n",
    "\n",
    "**Note: Could take several minutes if you don't have pre-built indices**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000, chunk_overlap=50, length_function=len, add_start_index=False\n",
    ")\n",
    "\n",
    "\n",
    "def split_and_prepare_document_new(qid: str, title: str, text: str):\n",
    "    split_text = text_splitter.split_text(text)\n",
    "    ids = [qid] * len(split_text)\n",
    "    titles = [title] * len(split_text)\n",
    "    embeddings = [\n",
    "        embedding_model.encode(xc, show_progress_bar=False) for xc in split_text\n",
    "    ]\n",
    "    return split_text, ids, titles, embeddings\n",
    "\n",
    "\n",
    "def process_batch(document_list):\n",
    "    batch_results = []\n",
    "\n",
    "    for id, title, text in zip(\n",
    "        document_list[\"id\"].values.tolist(),\n",
    "        document_list[\"title\"].values.tolist(),\n",
    "        document_list[\"text\"].values.tolist(),\n",
    "    ):\n",
    "        for sub_text, sub_id, sub_title, sub_embedding in zip(\n",
    "            *split_and_prepare_document_new(id, title, text)\n",
    "        ):\n",
    "            batch_results.append(tuple((sub_id, sub_title, sub_text, sub_embedding)))\n",
    "    return batch_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79afee15482746e39d0dd8676dabe70d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Documents in Batches:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed docs saved to pickle checkpoint\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "processed_docs = []\n",
    "cache_filename = '/Users/rrfsantos/Documents/GitHub/portifolio/prepared-docs.pkl'\n",
    "allow_cache = True\n",
    "\n",
    "if allow_cache and os.path.isfile(cache_filename):\n",
    "    os.remove(cache_filename)\n",
    "\n",
    "for i in tqdm(range(0, len(documents), batch_size), desc=\"Processing Documents in Batches\"):\n",
    "    # find end of batch\n",
    "    i_end = min(i + batch_size, len(documents))\n",
    "    documents_batch = documents[i:i_end]\n",
    "\n",
    "    # Process the batch\n",
    "    processed = process_batch(documents_batch)\n",
    "    processed_docs.extend(processed)\n",
    "\n",
    "with open(cache_filename, \"wb\") as f:\n",
    "    pickle.dump(processed_docs, f)\n",
    "\n",
    "print(\"Processed docs saved to pickle checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert the embeddings, texts, titles and documents id's in collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bbaa5ab928d48ddac274a9e321b5d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inserting documents batches to Milvus VectorDB:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if default_server.running:\n",
    "    collection = Collection(COLLECTION_NAME)\n",
    "    error  = []\n",
    "    batch_size = 1000\n",
    "    for i in tqdm(\n",
    "        range(0, len(processed_docs), batch_size),\n",
    "        desc=\"Inserting documents batches to Milvus VectorDB\",\n",
    "    ):\n",
    "        # find end of batch\n",
    "        i_end = min(i + batch_size, len(processed_docs))\n",
    "        id_l, title_l, text_l, embed_l = list(zip(*processed_docs[i:i_end]))\n",
    "\n",
    "        data_to_insert = [text_l, embed_l, id_l, title_l]\n",
    "        try:\n",
    "            collection.insert(data_to_insert)\n",
    "        except Exception as ex:\n",
    "            print(f\"Failed to insert: {ex}\")\n",
    "            error.append(title_l)\n",
    "else:\n",
    "    print(\"Milvus server is not running! Rerun related notebook cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection index has been successfully created!\n"
     ]
    }
   ],
   "source": [
    "NLIST_SIZE = 1024\n",
    "\n",
    "index_params = {\n",
    "    \"metric_type\": \"COSINE\",\n",
    "    \"index_type\": \"HNSW\",\n",
    "    \"params\": {\"nlist\": NLIST_SIZE},\n",
    "    \"M\": 16,\n",
    "    \"efConstruction\": 200,\n",
    "}\n",
    "\n",
    "collection.create_index(field_name=\"text_vector\", index_params=index_params)\n",
    "\n",
    "print(\"Collection index has been successfully created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"models\"></a>\n",
    "## Foundation Models on Watsonx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "BAM_API_URL = os.getenv(\"BAM_API_URL\")\n",
    "BAM_API_KEY = os.getenv(\"BAM_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salesforce/codegen2-16b\n",
      "codellama/codellama-34b-instruct\n",
      "tiiuae/falcon-180b\n",
      "tiiuae/falcon-40b\n",
      "ibm/falcon-40b-8lang-instruct\n",
      "google/flan-t5-xl\n",
      "google/flan-t5-xxl\n",
      "google/flan-ul2\n",
      "eleutherai/gpt-neox-20b\n",
      "ibm/granite-13b-chat-v1\n",
      "ibm/granite-13b-chat-v2\n",
      "ibm/granite-13b-instruct-v1\n",
      "ibm/granite-13b-instruct-v2\n",
      "ibm/granite-20b-code-instruct-v1\n",
      "ibm/granite-3b-code-plus-v1\n",
      "elyza/japanese-llama-2-7b-fast\n",
      "elyza/japanese-llama-2-7b-instruct\n",
      "meta-llama/llama-2-13b\n",
      "meta-llama/llama-2-13b-chat\n",
      "meta-llama/llama-2-13b-chat-beam\n",
      "meta-llama/llama-2-70b\n",
      "meta-llama/llama-2-70b-chat\n",
      "thebloke/llama-2-70b-chat-gptq\n",
      "meta-llama/llama-2-7b\n",
      "meta-llama/llama-2-7b-chat\n",
      "mosaicml/mpt-30b\n",
      "ibm/mpt-7b-instruct\n",
      "bigscience/mt0-xxl\n",
      "defog/sqlcoder-34b-alpha\n",
      "bigcode/starcoder\n",
      "flan-t5-xl-mpt-HrlayZEh-2023-10-25-18-15-34\n"
     ]
    }
   ],
   "source": [
    "# get the list of supported models from the API\n",
    "models = Model.models(credentials=credentials)\n",
    "\n",
    "model_ids = []\n",
    "for model_n in models:\n",
    "    print(model_n.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was found, it's token limit is 4096.\n"
     ]
    }
   ],
   "source": [
    "# select generative model to use\n",
    "model_id = \"tiiuae/falcon-180b\"\n",
    "\n",
    "# Iterate over the \"results\" list to find the matching model ID\n",
    "for model_n in models:\n",
    "    if model_n.id == model_id:\n",
    "        model_token_limit = model_n.token_limit\n",
    "        print(f\"Model was found, it's token limit is {model_token_limit}.\")\n",
    "        break\n",
    "else:\n",
    "    print(\"Model was not found, pick a different one!\")\n",
    "    model_token_limit = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up inference parameters\n",
    "params = GenerateParams(decoding_method=\"greedy\",\n",
    "                        max_new_tokens=500,\n",
    "                        min_new_tokens=10,\n",
    "                        repetition_penalty=1,\n",
    "                        temperature=0.5,\n",
    "                        top_k=50,\n",
    "                        top_p=1)\n",
    "\n",
    "model = Model(model=model_id, credentials=credentials, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input token limit depends on the selected generative model's max sequence length. The total input tokens in the RAG prompt should not exceed the model's max sequence length minus the number of desired output tokens. The choice of the number of paragraphs to retrieve as context impacts the number tokens in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token limit: 3595\n"
     ]
    }
   ],
   "source": [
    "# For setting the input token limit we subtract the max_new_tokens (to be generated) and -1 from the model_token_limit\n",
    "input_token_limit = model_token_limit - params.max_new_tokens - 1\n",
    "print(f\"Input token limit: {input_token_limit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"predict\"></a>\n",
    "## Generate a retrieval-augmented response to a question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Feed the context and the examples to `watsonx.ai` model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feed the context and the examples to `genai` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token counting function\n",
    "def token_count(doc):\n",
    "    return model.tokenize([doc])[0].token_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`prompt_template` is a function to create a prompt from the given context and question. Changing the prompt will sometimes result in much more appropriate answers (or it may degrade the quality significantly). The prompt template below is most appropriate for short-form extractive use cases.\n",
    "\n",
    "`make_prompt` includes a script to truncate the context length provided as an input in case the total token inputs exceed the model's limit. The paragraphs with the largest distance are truncated first. This functionality is helpful in case the embedded passages are not of the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = train_data.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_example=[]\n",
    "few_shot_examples=[]\n",
    "\n",
    "for index, row in examples.iterrows():\n",
    "    input = row.Question\n",
    "    output = row.answer_clean\n",
    "    few_shot_example.append(f\"input: {input}\\noutput: {output}\")\n",
    "few_shot_examples='\\n\\n\\n'.join(few_shot_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_template(context, few_shot_examples, question_text):\n",
    "    return ('''Below is an instruction that describes a task, paired with an input that provides further context and questions and answer examples.\n",
    "\n",
    "### Input:\n",
    "\n",
    "Based on the context and the examples, write a Response that appropriately answer the Question below.\n",
    "Provide your output in a maximum of 3 sentences.\n",
    "\n",
    "Context:'''\n",
    "            \n",
    "+ \"\\n\\n\"\n",
    "+ f\"{context}\\n\\n\" \n",
    "+ \"##\\n\\n\"\n",
    "+ \"Examples:\\n\\n\"\n",
    "+ f\"{few_shot_examples}\\n\\n\"\n",
    "+ f\"Question: {question_text}\\n\\n\"\n",
    "\n",
    "\"### Response:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(question, few_shot_examples, input_token_limit):\n",
    "\n",
    "    ### Create question embedding\n",
    "    question_embeddings = embedding_model.encode(question)\n",
    "\n",
    "    ### Collect the context in vetctordb\n",
    "    \n",
    "    search_params = {\"params\": {\"nprobe\": 10, \"radius\": 0.60}, \"metric_type\": \"COSINE\"} # radius ---> minimum similarity\n",
    "\n",
    "    results = collection.search(\n",
    "       data=[question_embeddings],\n",
    "       anns_field=\"text_vector\",\n",
    "       limit=2,\n",
    "       output_fields=[\"id\",\"text\"],  # name of the field to retrieve from the search result\n",
    "       param=search_params,\n",
    "       round_decimal=-1\n",
    "    )\n",
    "    \n",
    "    print(f\"Found {len(results[0])} results in the collection.\")\n",
    "\n",
    "    documents = []\n",
    "    for raw_result in results:\n",
    "        for result in raw_result:\n",
    "            documents.append(result.entity.get(\"text\"))\n",
    "            #print(\"=========\")\n",
    "            #print(\"Paragraph : \", result.entity.get(\"text\"))\n",
    "            print(\"Distance : \", result.distance)\n",
    "\n",
    "    context = \"\\n\\n\\n\".join(documents)\n",
    "\n",
    "    prompt = prompt_template(context, few_shot_examples, question)\n",
    "\n",
    "    prompt_token_count = token_count(prompt)\n",
    "\n",
    "    if prompt_token_count <= input_token_limit:\n",
    "        return prompt\n",
    "\n",
    "    print(\"exceeded input token limit, truncating context\", prompt_token_count)\n",
    "    \n",
    "    # documents with the lower distance scores are included in the truncated context first\n",
    "    distances = results[0].distances\n",
    "    sorted_indices = sorted(range(len(distances)), key=lambda k: distances[k])\n",
    "\n",
    "    truncated_context = \"\"\n",
    "    token_count_so_far = 0\n",
    "    i = 0\n",
    "\n",
    "    while token_count_so_far <= input_token_limit and i < len(sorted_indices):\n",
    "        doc_index = sorted_indices[i]\n",
    "        document = documents[doc_index]\n",
    "        doc_token_count = token_count(document) + token_count(few_shot_examples)\n",
    "\n",
    "        if token_count_so_far + doc_token_count <= input_token_limit:\n",
    "            truncated_context += document + \"\\n\\n\\n\"\n",
    "            token_count_so_far += doc_token_count\n",
    "        else:\n",
    "            remaining_tokens = input_token_limit - token_count_so_far\n",
    "            truncated_context += document[:remaining_tokens]\n",
    "            break\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return prompt_template(truncated_context, few_shot_examples, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 6.91 µs\n",
      "Found 2 results in the collection.\n",
      "Distance :  0.6770592927932739\n",
      "Distance :  0.652860164642334\n",
      "Found 0 results in the collection.\n",
      "Found 0 results in the collection.\n",
      "Found 2 results in the collection.\n",
      "Distance :  0.6616296172142029\n",
      "Distance :  0.6425576210021973\n",
      "Found 0 results in the collection.\n",
      "Found 0 results in the collection.\n",
      "Found 0 results in the collection.\n",
      "Found 2 results in the collection.\n",
      "Distance :  0.6613705158233643\n",
      "Distance :  0.6613705158233643\n",
      "Found 2 results in the collection.\n",
      "Distance :  0.641512930393219\n",
      "Distance :  0.6194244027137756\n",
      "Found 2 results in the collection.\n",
      "Distance :  0.7226449847221375\n",
      "Distance :  0.6797550320625305\n",
      "Found 0 results in the collection.\n",
      "Found 0 results in the collection.\n",
      "Found 0 results in the collection.\n",
      "Found 0 results in the collection.\n",
      "Found 0 results in the collection.\n",
      "Found 2 results in the collection.\n",
      "Distance :  0.6392431259155273\n",
      "Distance :  0.6392431259155273\n",
      "Found 0 results in the collection.\n",
      "Found 2 results in the collection.\n",
      "Distance :  0.7205710411071777\n",
      "Distance :  0.7062355875968933\n",
      "Found 0 results in the collection.\n",
      "Found 2 results in the collection.\n",
      "Distance :  0.6613160371780396\n",
      "Distance :  0.6600236892700195\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "questions = test_data.sample(n=20)\n",
    "questions = questions['Question'].tolist()\n",
    "collection = Collection(COLLECTION_NAME)      # Get the existing collection.\n",
    "collection.load() \n",
    "prompt_texts = []\n",
    "for question in questions:\n",
    "    prompt_text = make_prompt(question, few_shot_examples, input_token_limit)\n",
    "    prompt_texts.append(prompt_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 5.25 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "answers = []\n",
    "for response in model.generate(prompt_texts):\n",
    "    answers.append(response.generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context and questions and answer examples.\n",
      "\n",
      "### Input:\n",
      "\n",
      "Based on the context and the examples, write a Response that appropriately answer the Question below.\n",
      "Provide your output in a maximum of 3 sentences.\n",
      "\n",
      "Context:\n",
      "\n",
      "\n",
      "\n",
      "##\n",
      "\n",
      "Examples:\n",
      "\n",
      "input: Do many such drums have six to ten tension rods?\n",
      "output: yes\n",
      "\n",
      "\n",
      "input: Does Portuguese contain words from the Arabic language?\n",
      "output: yes\n",
      "\n",
      "\n",
      "input: Has Leonardo often been described as the archetype of the Renaissance man, a man whose unquenchable curiosity was equaled only by his powers of invention?\n",
      "output: yes\n",
      "\n",
      "\n",
      "input: Are vowels never reduced , regardless of stress ?\n",
      "output: yes\n",
      "\n",
      "\n",
      "input: Is the smallest penguin species the little blue penguin -LRB- also known as the fairy penguin -RRB- , which stands around 40 cm tall -LRB- 16 in -RRB- and weighs 1 kg -LRB- 2?\n",
      "output: yes\n",
      "\n",
      "\n",
      "input: Why are sea otters nearly extinct?\n",
      "output: they've been hunted for their fur\n",
      "\n",
      "\n",
      "input: Do some caterpillars produce vibrations as well as sounds?\n",
      "output: yes\n",
      "\n",
      "\n",
      "input: What is the second main orchestral use of cymbals?\n",
      "output: the suspended cymbal\n",
      "\n",
      "\n",
      "input: Why is Vattenfall shifting towards reliance on cleaner, renewable energy sources?\n",
      "output: because burning lignite produces harmful emissions, vattenfall has announced a commitment to shift towards reliance on cleaner, renewable energy sources\n",
      "\n",
      "\n",
      "input: When did the Gettysburg address argue that America was born?\n",
      "output: 1776\n",
      "\n",
      "Question: What are the ant colonies that lack queens called?\n",
      "\n",
      "### Response:\n"
     ]
    }
   ],
   "source": [
    "print(prompt_texts[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame({'Question': questions, 'Answer': answers})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the ant colonies that lack queens called?\n",
      "\n",
      "\n",
      "The ant colonies that lack queens are called gamergates.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_result.Question[10])\n",
    "print(df_result.Answer[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_server.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
